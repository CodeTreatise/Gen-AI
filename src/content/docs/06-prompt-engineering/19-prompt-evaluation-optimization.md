---
title: "Prompt Evaluation & Optimization"
---

# Prompt Evaluation & Optimization

- **Evaluation Fundamentals**
  - Define success criteria upfront
  - Quantitative metrics
  - Human evaluation criteria
  - Automated grading
- **OpenAI Evals System**
  - Create evals for tasks
  - `data_source_config` schema
  - `testing_criteria` graders
  - Dataset upload (JSONL format)
  - Run evaluations programmatically
- **Graders for Automated Testing**
  - `string_check` for exact match
  - Model-based graders
  - Custom grader logic
  - Multiple criteria per eval
- **Prompt Optimizer**
  - OpenAI Dashboard prompt optimizer
  - Prepare dataset with annotations
  - Good/Bad ratings
  - Text critiques for improvement
  - Automatic prompt refinement
- **Eval-Driven Development**
  - BDD-style prompt development
  - Write criteria first, then prompt
  - Iterate based on eval results
  - Regression testing for prompt changes
- **A/B Testing at Scale**
  - Compare prompt variants
  - Statistical significance
  - Multi-model comparison
  - Production traffic splitting
- **Prompt Debugging**
  - Identify failure modes
  - Edge case analysis
  - Gradual prompt refinement
  - Change impact tracking
