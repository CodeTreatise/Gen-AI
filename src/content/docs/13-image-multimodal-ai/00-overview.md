---
title: "Unit 13: Image & Multimodal AI"
---

# Unit 13: Image & Multimodal AI

## Overview & Importance

Multimodal AI works with multiple types of content â€” text, images, audio, and video. This unit covers image generation (creating images from text), image understanding (extracting information from images), and **video generation (NEW 2025-2026)**. These capabilities enable rich, visual AI applications that represent the cutting edge of generative AI.

**2025-2026 Evolution:**
- **Native Multimodal LLMs**: Models like `gpt-image-1.5` and Gemini's Nano Banana generate images directly from conversation, not as separate specialized models
- **Video Generation APIs**: OpenAI Sora and Google Veo 3.1 enable programmatic video creation with native audio
- **Enhanced Vision**: Object detection with bounding boxes (Gemini 2.0+), segmentation masks (Gemini 2.5+)
- **Unified APIs**: Responses API allows image generation as a tool within conversations
- **Higher Fidelity**: 4K image generation, improved text rendering, up to 14 reference images

Multimodal AI enables:
- Creating visual content programmatically (images and video)
- Understanding and analyzing images with structured outputs
- Building accessible applications (image descriptions, alt text)
- Richer, more natural human-AI interaction
- **Object detection and segmentation (NEW 2025)**
- **Video content creation and editing (NEW 2025)**
- **Real-time multimodal conversations (NEW 2025)**

## Prerequisites

- API integration skills (Unit 3)
- JavaScript fundamentals (Unit 1)
- Understanding of base64 encoding
- Basic file handling knowledge

## Learning Objectives

By the end of this unit, you will be able to:
- Generate images using AI APIs (Image API and Responses API)
- Send images to AI for analysis and understanding
- Handle image data in web applications
- Design effective prompts for image generation
- Extract structured data from images
- Build multimodal conversational interfaces
- Optimize image handling for performance
- **Use native LLM image generation (gpt-image-1, Nano Banana) (NEW 2025)**
- **Implement object detection with bounding boxes (NEW 2025)**
- **Apply segmentation masks for precise analysis (NEW 2025)**
- **Generate videos using Sora and Veo APIs (NEW 2025)**
- **Build multi-turn image editing workflows (NEW 2025)**
- **Control image fidelity and transparency settings (NEW 2025)**

## Real-world Applications

- AI art and design tools
- Product image generation for e-commerce
- Marketing content creation
- Document digitization and processing
- Accessibility tools (image descriptions)
- Visual search systems
- Medical image analysis interfaces
- Real estate virtual staging
- Fashion try-on applications
- **AI video content creation (ads, social media) (NEW 2025)**
- **Automated video editing and remixing (NEW 2025)**
- **Object detection for inventory/quality control (NEW 2025)**
- **Document segmentation and form processing (NEW 2025)**
- **Real-time visual assistance (live guidance) (NEW 2025)**
- **Conversational image design tools (NEW 2025)**
- **Brand asset generation at scale (NEW 2025)**

## Market Demand & Relevance

- Visual AI is rapidly growing market segment
- E-commerce heavily investing in image AI
- Design tools being disrupted by AI generation
- Document processing automation in demand
- Healthcare image analysis expanding
- Cross-skill value: web dev + visual AI
- Creative industries adopting AI tools
- **Video generation APIs enabling new content categories (2025)**
- **Native multimodal LLMs replacing specialized models (2025)**
- **Object detection/segmentation democratizing computer vision (2025)**
- **Real-time multimodal enabling live AI assistance (2025)**
- **Enterprise adoption of visual AI accelerating (2025-2026)**

## Resources & References

### Official Documentation
- [OpenAI Image Generation Guide](https://platform.openai.com/docs/guides/image-generation)
- [OpenAI Video Generation (Sora) Guide](https://platform.openai.com/docs/guides/video-generation)
- [Google Gemini Image Understanding](https://ai.google.dev/gemini-api/docs/image-understanding)
- [Google Nano Banana Image Generation](https://ai.google.dev/gemini-api/docs/image-generation)
- [Google Veo Video Generation](https://ai.google.dev/gemini-api/docs/video)
- [Anthropic Claude Vision](https://docs.anthropic.com/en/docs/build-with-claude/vision)

### Key Concepts
- [OpenAI Responses API](https://platform.openai.com/docs/guides/responses-api)
- [Gemini Media Resolution](https://ai.google.dev/gemini-api/docs/media-resolution)
- [Object Detection Cookbook](https://github.com/google-gemini/cookbook)
- [Segmentation Quickstart](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb)

### Model References
- **OpenAI**: gpt-image-1.5, gpt-image-1, gpt-image-1-mini, sora-2, sora-2-pro
- **Google**: gemini-2.5-flash-image, gemini-3-pro-image-preview, veo-3.1-generate-preview, imagen-4
- **Anthropic**: claude-4-* (vision capabilities)

### Tutorials & Guides
- [Sora Prompting Guide](https://cookbook.openai.com/examples/sora/sora2_prompting_guide)
- [Veo Prompt Guide](https://ai.google.dev/gemini-api/docs/video#veo-prompt-guide)
- [Gemini Cookbook](https://github.com/google-gemini/cookbook)
