---
title: "Multimodal Embeddings"
---

# Multimodal Embeddings

- What are multimodal embeddings?
  - Single vector space for multiple modalities
  - Text and images in same embedding space
  - Enable cross-modal search
  - Foundation for multimodal RAG
- Cohere embed-v4.0 (2025 state-of-art)
  - Text + image embeddings unified
  - Mixed content support (text + image together)
  - 100+ languages supported
  - Matryoshka dimensions: 256, 512, 1024, 1536
  - Up to 96 images per batch
- Image embedding with Cohere
  - Input types: image, search_document, search_query
  - Supported formats: PNG, JPEG, WebP, GIF
  - Max size: 5MB per image
  - Base64 Data URL encoding
  - Image resolution handling (auto-resize)
- Mixed content embeddings
  - Combine text and images in single embedding
  - Use for slides, documents with figures
  - Eliminates complex ETL pipelines
  - Single vector represents full content
- CLIP and alternatives
  - OpenAI CLIP (open source)
  - Google multimodal embeddings
  - Open source: SigLIP, OpenCLIP
  - Trade-offs: quality vs availability
- Use cases for multimodal
  - Image search with text queries
  - Product search (image + description)
  - Document understanding (figures + text)
  - Visual Q&A systems
  - Content-based recommendations
- Implementation considerations
  - Separate indices for text vs image?
  - Unified index for cross-modal search
  - Storage requirements (images larger)
  - Latency for image processing
