---
title: "Contextual Retrieval (Anthropic Method)"
---

# Contextual Retrieval (Anthropic Method)

- The context problem in RAG
  - Chunks lose document context
  - "Revenue grew 3%" - which company? when?
  - Retrieval fails without context
  - Even good embeddings can't fix this
- Contextual Retrieval solution
  - Prepend explanatory context to each chunk
  - Context generated by LLM (Claude)
  - 50-100 tokens of context per chunk
  - Applied before embedding AND before BM25
- The contextualizer prompt
  - Provide full document
  - Provide specific chunk
  - Ask for succinct situating context
  - Example: "This chunk is from Q2 2023 SEC filing for ACME Corp..."
- Implementation steps
  - 1. Split document into chunks
  - 2. For each chunk, generate context (with full doc)
  - 3. Prepend context to chunk
  - 4. Create embedding of contextualized chunk
  - 5. Create BM25 index of contextualized chunks
- Performance improvements (Anthropic research)
  - Contextual Embeddings alone: 35% fewer failures
  - Contextual Embeddings + Contextual BM25: 49% fewer failures
  - + Reranking: 67% fewer failures
  - Tested across codebases, papers, fiction, SEC filings
- Cost optimization with prompt caching
  - Document loaded once into cache
  - Each chunk contextualization reuses cache
  - ~$1.02 per million document tokens
  - Essential for cost-effective implementation
- Best practices
  - Use with Voyage or Gemini embeddings (best results)
  - Fetch top-20 chunks (better than 5 or 10)
  - Combine with reranking for best results
  - Custom prompts for specific domains
  - Preserve context vs chunk distinction in retrieval
