---
title: "What Are Multimodal Embeddings?"
---

# What Are Multimodal Embeddings?

## Introduction

Traditional embeddings work with a single modalityâ€”text goes in, text embeddings come out. **Multimodal embeddings** break this barrier by projecting multiple data typesâ€”text, images, video, audioâ€”into the **same vector space**.

This means a picture of a cat and the text "a fluffy orange cat" will have embeddings that are **close together** in vector space, enabling entirely new search and retrieval patterns.

### What We'll Cover

- The unified vector space concept
- How cross-modal search works
- Training approaches for multimodal models
- Why this matters for RAG and search

### Prerequisites

- [Understanding Embeddings](../01-understanding-embeddings/)
- Basic understanding of neural networks

---

## The Unified Vector Space Concept

### Traditional Embeddings: Separate Spaces

With traditional models, different data types exist in incompatible spaces:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 TRADITIONAL EMBEDDINGS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   TEXT SPACE                      IMAGE SPACE                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚  "sunset"   â”‚     â›”          â”‚  ğŸŒ… [img]   â”‚               â”‚
â”‚   â”‚  "beach"    â”‚   INCOMPATIBLE  â”‚  ğŸ–ï¸ [img]   â”‚               â”‚
â”‚   â”‚  "ocean"    â”‚                 â”‚  ğŸŒŠ [img]   â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                 â”‚
â”‚   âŒ Cannot compare text to images                              â”‚
â”‚   âŒ Separate indexes required                                  â”‚
â”‚   âŒ No cross-modal search                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multimodal Embeddings: Unified Space

Multimodal models align text and images in the same geometric space:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 MULTIMODAL EMBEDDINGS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚                    UNIFIED VECTOR SPACE                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚                                                 â”‚           â”‚
â”‚   â”‚          "sunset at the beach"  â†â”€â”€â”€â”          â”‚           â”‚
â”‚   â”‚                  â€¢                   â”‚          â”‚           â”‚
â”‚   â”‚              â€¢  ğŸŒ…                   â”‚ CLOSE    â”‚           â”‚
â”‚   â”‚                  â€¢ ğŸ–ï¸                â”‚          â”‚           â”‚
â”‚   â”‚                                      â”‚          â”‚           â”‚
â”‚   â”‚     "mountain landscape"  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”  â”‚           â”‚
â”‚   â”‚            â€¢                         â”‚       â”‚  â”‚           â”‚
â”‚   â”‚        â€¢ ğŸ”ï¸                         â”‚ CLOSE â”‚  â”‚           â”‚
â”‚   â”‚            â€¢ ğŸ—»                       â”‚       â”‚  â”‚           â”‚
â”‚   â”‚                                             â”‚  â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                 â”‚
â”‚   âœ… Text and images comparable directly                        â”‚
â”‚   âœ… Single unified index                                       â”‚
â”‚   âœ… Cross-modal search enabled                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## How It Works: Contrastive Learning

### The Training Objective

Multimodal models are typically trained using **contrastive learning** on paired data (e.g., images with captions):

```
Training Data:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŒ… sunset_beach.jpg  â†”  "Beautiful sunset"  â”‚
â”‚ ğŸ± cat_sleeping.jpg  â†”  "Cat napping"       â”‚
â”‚ ğŸš— red_car.jpg       â†”  "Red sports car"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Training Objective:
â€¢ MAXIMIZE similarity between paired image-text
â€¢ MINIMIZE similarity between non-paired items
```

### Contrastive Loss Visualization

```python
# Simplified contrastive learning concept
def contrastive_loss(image_embeddings, text_embeddings):
    """
    For a batch of N image-text pairs:
    - Diagonal elements (matched pairs) should be HIGH similarity
    - Off-diagonal elements (mismatched) should be LOW similarity
    """
    # Compute similarity matrix (N x N)
    similarity_matrix = image_embeddings @ text_embeddings.T
    
    # Target: identity matrix (only diagonal = 1)
    # Loss encourages:
    #   similarity[i,i] â†’ HIGH (matched pairs)
    #   similarity[i,j] â†’ LOW  (mismatched, iâ‰ j)
    
    labels = torch.arange(N)
    loss_i2t = cross_entropy(similarity_matrix, labels)  # Image-to-text
    loss_t2i = cross_entropy(similarity_matrix.T, labels)  # Text-to-image
    
    return (loss_i2t + loss_t2i) / 2
```

### The Result: Aligned Spaces

After training:

| Query | Similar Results |
|-------|-----------------|
| Text: "a dog playing fetch" | Images of dogs playing + related captions |
| Image: ğŸ• (dog photo) | Similar dog images + "dog playing" text |
| Text: "product packaging design" | Product images + design descriptions |

---

## Cross-Modal Search Patterns

### 1. Text-to-Image Search

The most common pattern: user types text, finds relevant images.

```python
# User query (text)
query = "modern minimalist living room"
query_embedding = embed_text(query)

# Search image index
results = vector_db.search(
    query_embedding,
    collection="interior_design_images"
)

# Returns: Living room images matching the description
```

### 2. Image-to-Text Search

Find text content related to an image:

```python
# User uploads image
image_embedding = embed_image(user_uploaded_image)

# Search text index
results = vector_db.search(
    image_embedding,
    collection="product_descriptions"
)

# Returns: Text descriptions matching the image
```

### 3. Image-to-Image Search

Find visually similar images:

```python
# Reference image
reference_embedding = embed_image(reference_image)

# Search image index
similar_images = vector_db.search(
    reference_embedding,
    collection="image_library"
)
```

### 4. Unified Search

Search across all content types simultaneously:

```python
# Text query
query_embedding = embed_text("sunset photography")

# Search unified index containing both text and images
results = vector_db.search(
    query_embedding,
    collection="all_content"  # Mixed text + images
)

# Returns: Sunset images AND articles about sunset photography
```

---

## The Semantic Bridge

### Why Same Space Matters

The key insight: embeddings measure **semantic meaning**, not surface form.

| Concept | Text Representation | Image Representation |
|---------|---------------------|----------------------|
| "Dog" | Word vectors | Pixel patterns |
| "Happy" | Contextual semantics | Facial expressions, body language |
| "Luxury car" | Brand associations | Visual design cues |

Multimodal models learn that these different representations **refer to the same concept**.

### Geometric Interpretation

```
Vector Space Geometry:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚   "sports car"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—                                  â”‚
â”‚                              â”‚                                  â”‚
â”‚   ğŸš— [ferrari.jpg]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â†â”€â”€ These cluster together        â”‚
â”‚                              â”‚                                  â”‚
â”‚   ğŸï¸ [porsche.jpg]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â—                                  â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚   "flower garden"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â—                                   â”‚
â”‚                              â”‚                                  â”‚
â”‚   ğŸŒ¸ [roses.jpg]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â†â”€â”€ These cluster together         â”‚
â”‚                              â”‚                                  â”‚
â”‚   ğŸŒº [tulips.jpg]  â”€â”€â”€â”€â”€â”€â”€â”€â”€â—                                   â”‚
â”‚                                                                 â”‚
â”‚   Distance(sports_car_text, ferrari_image) < 0.2                â”‚
â”‚   Distance(sports_car_text, roses_image) > 0.9                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Foundation for Multimodal RAG

### Traditional RAG: Text Only

```
User Query (text) â†’ Text Embeddings â†’ Text Chunks â†’ LLM â†’ Answer
```

### Multimodal RAG: Text + Images

```
User Query (text) â†’ Multimodal Embedding â†’ Text Chunks + Images â†’ Vision LLM â†’ Answer with Visual Context
```

### Example: Product Support

```python
# User asks: "How do I install the wall mount bracket?"

# Traditional RAG: Find text instructions only
text_results = search_text_index(query)

# Multimodal RAG: Find text AND relevant diagrams
multimodal_results = search_unified_index(query)
# Returns: Installation text + diagram images

# Send to vision-capable LLM
response = vision_llm.generate(
    query=query,
    context=multimodal_results  # Includes images!
)
# "To install the wall mount bracket, first locate the 
#  mounting holes as shown in this diagram [image]..."
```

---

## Limitations and Considerations

### Current Limitations

| Limitation | Details |
|------------|---------|
| **Text length** | Often limited (32-77 tokens for CLIP-based models) |
| **Image resolution** | Typically resized to 224Ã—224 or 512Ã—512 |
| **Language support** | Many models English-only or limited multilingual |
| **Abstract concepts** | Better at concrete objects than abstract ideas |
| **OCR text** | May confuse text IN images with image content |

### When Multimodal Helps

âœ… Image search with natural language queries  
âœ… Product catalogs with visual and text content  
âœ… Document understanding (text + figures)  
âœ… Visual Q&A systems  
âœ… Content-based recommendations  

### When to Stick with Text-Only

âœ… Pure text retrieval (documents, articles)  
âœ… Long-form content (multimodal often truncates text)  
âœ… Non-visual domains (legal, financial text)  
âœ… When image processing latency is prohibitive  

---

## Evolution of Multimodal Models

### Brief History

| Year | Model | Significance |
|------|-------|-------------|
| 2021 | CLIP (OpenAI) | First widely-used contrastive multimodal model |
| 2021 | ALIGN (Google) | Scaled to 1.8B image-text pairs |
| 2023 | SigLIP | Improved CLIP with sigmoid loss |
| 2024 | Cohere embed-v3.0 | Production API with text+image |
| 2025 | Cohere embed-v4.0 | Mixed content, 100+ languages |

### Model Architecture (Simplified)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTIMODAL MODEL                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   TEXT INPUT                         IMAGE INPUT                â”‚
â”‚       â”‚                                   â”‚                     â”‚
â”‚       â–¼                                   â–¼                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚  Text   â”‚                       â”‚  Image  â”‚                 â”‚
â”‚   â”‚ Encoder â”‚                       â”‚ Encoder â”‚                 â”‚
â”‚   â”‚(Transf.)â”‚                       â”‚ (ViT)   â”‚                 â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                 â”‚
â”‚        â”‚                                  â”‚                     â”‚
â”‚        â–¼                                  â–¼                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚Projectionâ”‚                      â”‚Projectionâ”‚                â”‚
â”‚   â”‚  Layer   â”‚                      â”‚  Layer   â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                 â”‚
â”‚        â”‚                                  â”‚                     â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                       â–¼                                         â”‚
â”‚              SHARED EMBEDDING SPACE                             â”‚
â”‚                 (Same dimensions)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

âœ… Multimodal embeddings place **text and images in the same vector space**  
âœ… **Contrastive learning** on paired data creates semantic alignment  
âœ… Enables **cross-modal search**: textâ†’image, imageâ†’text, imageâ†’image  
âœ… Foundation for **multimodal RAG** with visual context  
âœ… Current models have limitations (text length, resolution, language)  
âœ… Choose multimodal when visual content matters for your use case

---

**Next:** [Cohere embed-v4.0 â†’](./02-cohere-embed-v4.md)

---

<!-- 
Sources Consulted:
- OpenAI CLIP paper and blog: https://openai.com/index/clip/
- Cohere Multimodal Embeddings: https://docs.cohere.com/docs/multimodal-embeddings
- Contrastive learning research literature
-->
