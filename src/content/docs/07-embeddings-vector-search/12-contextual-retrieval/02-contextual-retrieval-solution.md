---
title: "Contextual Retrieval Solution"
---

# Contextual Retrieval Solution

## Introduction

Contextual Retrieval solves the context loss problem by **prepending chunk-specific explanatory context** to each chunk before embedding. This context is generated by an LLM that has access to the full document.

This lesson explains the core technique and how it transforms retrieval accuracy.

### What We'll Cover

- The Contextual Retrieval approach
- How context is generated
- Before and after examples
- Why it works so well

### Prerequisites

- [The Context Problem](./01-the-context-problem.md)
- Basic understanding of LLM APIs

---

## The Core Insight

### Traditional vs Contextual

```
┌─────────────────────────────────────────────────────────────────┐
│              The Contextual Retrieval Transformation             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  BEFORE (Traditional):                                          │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ "The company's revenue grew by 3% over the previous     │   │
│  │  quarter."                                               │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Problem: No idea which company or when                         │
│                                                                 │
│  ─────────────────────────────────────────────────────────────  │
│                                                                 │
│  AFTER (Contextual):                                            │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ "This chunk is from an SEC filing on ACME Corp's        │   │
│  │  performance in Q2 2023; the previous quarter's         │   │
│  │  revenue was $314 million.                              │   │
│  │                                                          │   │
│  │  The company's revenue grew by 3% over the previous     │   │
│  │  quarter."                                               │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  Now contains: Company + Quarter + Year + Prior context         │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## How It Works

### The Process

1. **Split document** into chunks (as usual)
2. **For each chunk**, send the FULL document + that specific chunk to an LLM
3. **LLM generates** 50-100 tokens of situating context
4. **Prepend context** to the original chunk
5. **Embed** the contextualized chunk
6. **Index** contextualized chunks in both vector store AND BM25

```
┌─────────────────────────────────────────────────────────────────┐
│              Contextual Retrieval Pipeline                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Step 1: Chunk Document                                         │
│  ┌──────────────────────┐                                       │
│  │    Full Document     │                                       │
│  │    (8,000 tokens)    │                                       │
│  └──────────────────────┘                                       │
│            │                                                    │
│            ▼                                                    │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐                           │
│  │ C1 │ │ C2 │ │ C3 │ │ C4 │ │ C5 │  (10 chunks)              │
│  └────┘ └────┘ └────┘ └────┘ └────┘                           │
│                                                                 │
│  Step 2: Generate Context (for each chunk)                      │
│  ┌──────────────────────────────────────────────┐              │
│  │              LLM (Claude)                     │              │
│  │  Input: Full Document + Chunk C1             │              │
│  │  Output: "This chunk is from ACME Corp's..." │              │
│  └──────────────────────────────────────────────┘              │
│                                                                 │
│  Step 3: Create Contextualized Chunks                           │
│  ┌───────────────────────────────────────────┐                 │
│  │ Context: "This chunk is from ACME..."     │                 │
│  │ ─────────────────────────────────────────│                 │
│  │ Original: "The company's revenue grew..." │                 │
│  └───────────────────────────────────────────┘                 │
│                                                                 │
│  Step 4: Embed & Index                                          │
│  ┌────────────────┐    ┌────────────────┐                      │
│  │ Vector Index   │    │  BM25 Index    │                      │
│  │ (embeddings)   │    │ (keywords)     │                      │
│  └────────────────┘    └────────────────┘                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Context Generation

### What the LLM Produces

The LLM generates **succinct, chunk-specific context** (typically 50-100 tokens):

```python
# Example contextualizer output for different chunks

# Chunk from SEC filing
context_1 = """This chunk is from an SEC filing on ACME Corp's 
performance in Q2 2023; the previous quarter's revenue was 
$314 million."""

# Chunk from technical documentation
context_2 = """This chunk describes the deprecated 'distutils' 
module in Python 3.12, specifically discussing migration paths 
to setuptools."""

# Chunk from research paper
context_3 = """This chunk is from the 'Attention Is All You Need' 
paper by Vaswani et al. (2017), describing the scaled dot-product 
attention mechanism in the Transformer architecture."""
```

### Context Characteristics

| Characteristic | Description |
|----------------|-------------|
| **Length** | 50-100 tokens (short and focused) |
| **Specificity** | Unique to THIS chunk, not generic |
| **Information** | Entity, temporal, structural context |
| **Format** | Plain text, prepended to chunk |

---

## Before and After Examples

### Example 1: Financial Document

**Original Chunk:**
```
"The company's revenue grew by 3% over the previous quarter. 
Operating expenses decreased by 5%, contributing to improved 
net income margins."
```

**Contextualized Chunk:**
```
This chunk is from ACME Corporation's Q2 2023 SEC filing 
(Form 10-Q). The document reports financial performance for 
the quarter ending June 30, 2023. Previous quarter revenue 
was $314 million.

The company's revenue grew by 3% over the previous quarter. 
Operating expenses decreased by 5%, contributing to improved 
net income margins.
```

**What's Added:**
- ✅ Company name (ACME Corporation)
- ✅ Document type (SEC filing, Form 10-Q)
- ✅ Time period (Q2 2023)
- ✅ Exact date (quarter ending June 30, 2023)
- ✅ Reference point (previous quarter = $314M)

### Example 2: Technical Documentation

**Original Chunk:**
```
"The module provides utilities for building and distributing 
Python packages. It has been deprecated and will be removed 
in Python 3.14."
```

**Contextualized Chunk:**
```
This chunk describes the distutils module in the Python 3.12 
documentation, under the section "Deprecated and Pending 
Deprecation". The module was part of the standard library 
since Python 1.6.

The module provides utilities for building and distributing 
Python packages. It has been deprecated and will be removed 
in Python 3.14.
```

**What's Added:**
- ✅ Module name (distutils)
- ✅ Python version (3.12)
- ✅ Section location (Deprecated and Pending Deprecation)
- ✅ Historical context (part of stdlib since 1.6)

### Example 3: Research Paper

**Original Chunk:**
```
"We employ attention mechanisms to compute representations 
of its input and output. We use self-attention to relate 
different positions of a single sequence."
```

**Contextualized Chunk:**
```
This chunk is from Section 3 (Model Architecture) of 
"Attention Is All You Need" by Vaswani et al., published 
at NeurIPS 2017. This paper introduced the Transformer 
architecture which revolutionized NLP.

We employ attention mechanisms to compute representations 
of its input and output. We use self-attention to relate 
different positions of a single sequence.
```

**What's Added:**
- ✅ Paper title and authors
- ✅ Section (Model Architecture)
- ✅ Publication venue (NeurIPS 2017)
- ✅ Historical significance

---

## Why This Works

### Embeddings Now Capture Context

```python
# Before: Query and chunk may not match on key terms
query = "ACME Corp Q2 2023 revenue growth"
chunk = "The company's revenue grew by 3%"  # No "ACME", no "Q2"

# After: Context adds matchable terms
contextualized_chunk = """
This chunk is from ACME Corporation's Q2 2023 SEC filing...
The company's revenue grew by 3%...
"""
# Now contains "ACME", "Q2", "2023" - matches query better!
```

### BM25 Keyword Matching

The added context also helps lexical search (BM25):

| Query Term | In Original? | In Contextualized? |
|------------|--------------|-------------------|
| "ACME" | ❌ No | ✅ Yes |
| "Q2" | ❌ No | ✅ Yes |
| "2023" | ❌ No | ✅ Yes |
| "SEC" | ❌ No | ✅ Yes |
| "revenue" | ✅ Yes | ✅ Yes |
| "growth" | ❌ No | ✅ Yes (or "grew") |

---

## Implementation Overview

### Basic Structure

```python
import anthropic

def generate_context(document: str, chunk: str) -> str:
    """Generate situating context for a chunk."""
    client = anthropic.Anthropic()
    
    prompt = f"""<document>
{document}
</document>

Here is the chunk we want to situate within the whole document:
<chunk>
{chunk}
</chunk>

Please give a short succinct context to situate this chunk within 
the overall document for the purposes of improving search retrieval 
of the chunk. Answer only with the succinct context and nothing else."""

    response = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=200,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response.content[0].text


def contextualize_chunk(document: str, chunk: str) -> str:
    """Create contextualized chunk by prepending context."""
    context = generate_context(document, chunk)
    return f"{context}\n\n{chunk}"
```

### Processing a Document

```python
def process_document(document: str, chunk_size: int = 500) -> list[str]:
    """Process document into contextualized chunks."""
    # 1. Split into chunks
    chunks = split_into_chunks(document, chunk_size)
    
    # 2. Generate context for each chunk
    contextualized_chunks = []
    for chunk in chunks:
        ctx_chunk = contextualize_chunk(document, chunk)
        contextualized_chunks.append(ctx_chunk)
    
    return contextualized_chunks


# Example usage
document = load_document("acme_q2_2023_10q.txt")
contextualized = process_document(document)

print(f"Original chunks: {len(chunks)}")
print(f"Contextualized chunks: {len(contextualized)}")
print(f"\nExample contextualized chunk:\n{contextualized[0]}")
```

---

## Key Differentiators

### vs. Generic Document Summaries

| Approach | How It Works | Effectiveness |
|----------|--------------|---------------|
| Generic Summary | Same summary prepended to ALL chunks | ❌ Limited gains |
| Contextual Retrieval | **Unique context per chunk** | ✅ 35-67% improvement |

The key difference: Context is **specific to each chunk**, not a one-size-fits-all summary.

### vs. Hypothetical Document Embedding (HyDE)

| Approach | Process | Use Case |
|----------|---------|----------|
| HyDE | Generate hypothetical doc for query | Query expansion |
| Contextual Retrieval | Add context to documents | Document preprocessing |

Contextual Retrieval operates at **indexing time** (preprocessing), not query time.

---

## Cost Considerations

### Without Optimization

```
10 chunks × 8,000 doc tokens = 80,000 input tokens per document

At Claude Haiku rates (~$0.25/M input):
- 1 document: ~$0.02
- 1,000 documents: ~$20
- 100,000 documents: ~$2,000
```

### With Prompt Caching

```
Document cached once, each chunk generation reuses cache

Cost: ~$1.02 per million document tokens
- 1,000 documents (8K tokens each): ~$8
- 90% cost reduction!
```

We'll cover prompt caching implementation in [Lesson 07](./07-prompt-caching.md).

---

## Summary

✅ **Contextual Retrieval prepends LLM-generated context** to each chunk  
✅ Context is **50-100 tokens, chunk-specific** (not generic)  
✅ Adds **entity, temporal, structural information** missing from chunks  
✅ Improves both **embedding similarity AND keyword matching**  
✅ Works at **indexing time** (preprocessing, not query time)  
✅ **Prompt caching** makes it cost-effective at scale

---

**Next:** [The Contextualizer Prompt →](./03-the-contextualizer-prompt.md)

---

<!-- 
Sources Consulted:
- Anthropic Contextual Retrieval: https://www.anthropic.com/news/contextual-retrieval
-->
