---
title: "Embedding Models"
---

# Embedding Models

- Text embedding models (2025 landscape)
  - OpenAI text-embedding-3-small (1536d) / large (3072d)
  - OpenAI dimensions parameter for flexible output
  - Cohere embed-v4.0 (multimodal, Matryoshka)
  - Voyage AI voyage-3, voyage-code-3
  - Google gemini-embedding-001 (3072d, June 2025)
  - Anthropic (no public embedding model - uses partners)
- Model specifications and benchmarks
  - OpenAI text-embedding-3-large: MTEB 64.6
  - Gemini embedding: 768d=67.99, 1536d=68.17, 3072d=68.16
  - Cohere embed-v4.0: multilingual + multimodal
  - Context lengths: OpenAI 8192, Gemini 2048 tokens
- Open-source embedding models
  - Sentence Transformers (all-MiniLM, all-mpnet)
  - BGE embeddings (BAAI/bge-large-en-v1.5)
  - E5 models (intfloat/e5-large-v2)
  - GTE models (Alibaba)
  - Nomic embed (open weights, long context)
  - Hugging Face MTEB leaderboard
- Model dimensions and quality trade-offs
  - Smaller models (384-768d): fast, cheap, good for simple tasks
  - Medium models (1024-1536d): balanced performance
  - Larger models (3072d): highest quality, more storage
  - Matryoshka models: choose dimension at query time
  - Benchmark comparisons (MTEB, BEIR)
  - Task-specific performance varies significantly
- Specialized embeddings
  - Code embeddings: voyage-code-3, CodeBERT
  - Multilingual: embed-v4.0 (100+ languages), multilingual-e5
  - Domain-specific fine-tuned models
  - Cross-lingual retrieval capabilities
  - Legal, medical, financial domain models
- Embedding model selection criteria
  - Use case: search vs. classification vs. clustering
  - Language support requirements
  - Context length support (2K vs 8K tokens)
  - Latency requirements (local vs API)
  - Multimodal needs (text + images)
- Cost considerations (2025 pricing)
  - OpenAI: $0.02/1M tokens (small), $0.13/1M (large)
  - Gemini: $0.15/1M tokens, batch 50% discount
  - Cohere: tiered pricing with free tier
  - Self-hosting: GPU costs vs API costs
  - Caching strategies for cost reduction
