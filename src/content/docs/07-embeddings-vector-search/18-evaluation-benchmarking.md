---
title: "Evaluation & Benchmarking"
---

# Evaluation & Benchmarking

- Retrieval quality metrics
  - Recall@k: relevant docs in top-k
  - Precision@k: relevant ratio in top-k
  - MRR (Mean Reciprocal Rank)
  - NDCG (Normalized Discounted Cumulative Gain)
  - Hit rate: any relevant in top-k
- MTEB benchmark
  - Massive Text Embedding Benchmark
  - Tasks: retrieval, classification, clustering, etc.
  - Compare models on standard datasets
  - Limitations: may not match your domain
- BEIR benchmark
  - Diverse retrieval benchmark
  - 18 datasets across domains
  - Zero-shot retrieval evaluation
  - Industry standard for RAG models
- Building evaluation datasets
  - Collect query-document relevance pairs
  - Synthetic data generation with LLMs
  - Human annotation guidelines
  - Minimum dataset size recommendations
- A/B testing retrieval
  - Embedding model A vs B
  - Chunking strategy comparison
  - Reranking impact measurement
  - Statistical significance in search
- End-to-end RAG evaluation
  - Retrieval quality vs generation quality
  - Faithfulness: answers grounded in retrieved docs
  - Context relevance: retrieved docs match query
  - Answer relevance: answer addresses query
  - Tools: RAGAS, TruLens, DeepEval
- Continuous monitoring
  - Drift detection in queries
  - Quality degradation alerts
  - Feedback loop from users
  - Regular re-evaluation cadence
