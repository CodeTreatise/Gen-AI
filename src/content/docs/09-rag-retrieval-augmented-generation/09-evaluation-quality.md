---
title: "Evaluation & Quality"
---

# Evaluation & Quality

- RAG-specific metrics
  - Context relevance
  - Answer faithfulness
  - Answer relevance
  - Combined quality scores
- Retrieval quality evaluation (recall, precision)
  - Recall: finding all relevant
  - Precision: avoiding noise
  - F1 score
  - Mean Reciprocal Rank (MRR)
- Answer quality evaluation
  - Correctness assessment
  - Completeness evaluation
  - Coherence scoring
  - Helpfulness ratings
- Faithfulness checking
  - LLM-as-judge evaluation
  - NLI-based checking
  - Claim extraction
  - Automated faithfulness tools
- Ground truth testing
  - Golden dataset creation
  - Expected answer comparison
  - Rubric-based scoring
  - Regression testing
- User feedback loops
  - Thumbs up/down
  - Detailed feedback forms
  - Implicit feedback (clicks)
  - Feedback analysis
- RAGAS Evaluation Framework
  - Industry-standard RAG evaluation library
  - Core retrieval metrics:
    - **Context Precision**: Are retrieved chunks relevant to the query?
    - **Context Recall**: Did we retrieve all relevant information?
    - **Context Entities Recall**: Are key entities from ground truth retrieved?
    - **Noise Sensitivity**: How robust to irrelevant retrieved content?
  - Core generation metrics:
    - **Faithfulness**: Is the answer grounded in retrieved context?
    - **Response Relevancy**: Does the answer address the query?
    - **Factual Correctness**: Are claims in the answer accurate?
  - Basic RAGAS usage:
    - Import `evaluate` function from ragas module
    - Import individual metrics: context_precision, context_recall, faithfulness, answer_relevancy
    - Call `evaluate()` with dataset parameter containing test samples
    - Pass metrics parameter as list of metric functions to compute
    - Result is a DataFrame with scores per sample for each metric
    - Each row represents one test case, columns are metric scores
    - Metrics are computed using LLM-as-judge internally
  - Agentic evaluation metrics:
    - Topic Adherence
    - Tool Call Accuracy
    - Tool Call F1
    - Agent Goal Accuracy
  - Test data generation for RAG evaluation
  - Integration with LangSmith, Arize, Bedrock
- The RAG Triad (industry standard)
  - Three core quality dimensions:
    1. **Context Relevance**: Retrieved context matches query intent
    2. **Groundedness**: Response is supported by retrieved context
    3. **Answer Relevance**: Response addresses the original question
  - Triad ensures end-to-end quality
  - Each dimension can be evaluated independently
  - Combined score for overall RAG quality
- LLM-as-Judge evaluation patterns
  - Pairwise comparison (A vs B)
  - Rubric-based scoring (1-5 scale)
  - Aspect-based evaluation (multiple criteria)
  - Self-consistency checking
  - Calibration with human judgments
