---
title: "Rate Limiting"
---

# Rate Limiting

## Introduction

An AI agent in an agentic loop can call tools dozens of times per conversation ‚Äî and every call consumes resources: API quota, database connections, compute cycles, and money. Without rate limiting, a single runaway conversation can exhaust your API rate limit, overload your database, or rack up unexpected charges.

Rate limiting for AI tool calling operates at multiple levels: per-tool, per-user, per-conversation, and per-time-window. We also need to consider **token budgets** ‚Äî since tool definitions and results consume input tokens against your context window.

### What we'll cover

- Per-tool and per-user rate limiting
- Conversation-level call budgets
- Token budget management
- Backpressure and graceful degradation
- Implementing rate limiters for tool handlers

### Prerequisites

- [Safe Defaults](./04-safe-defaults.md) ‚Äî Defensive tool design
- [Security Best Practices](./07-security-best-practices.md) ‚Äî Authorization patterns

---

## Why rate limiting matters for AI tools

Unlike traditional APIs where each request comes from a user action (click, form submit), AI tool calls are generated by the model automatically. A model in an agentic loop might:

- Call `search_products` 50 times trying to find the right item
- Retry a failed `send_email` indefinitely
- Call `get_order` for every order in a customer's history sequentially
- Generate parallel tool calls that all hit the same database

Each of these scenarios needs rate limiting to prevent resource exhaustion.

---

## Per-tool rate limits

Different tools have different costs. A read-only search is cheap; a payment processing call is expensive:

```python
from datetime import datetime, timedelta
from collections import defaultdict
from dataclasses import dataclass, field


@dataclass
class RateLimit:
    max_calls: int          # Maximum calls allowed
    window_seconds: int     # Time window in seconds
    
    def __str__(self):
        return f"{self.max_calls} calls per {self.window_seconds}s"


# Define limits per tool based on cost and risk
TOOL_RATE_LIMITS: dict[str, RateLimit] = {
    # Read operations ‚Äî generous limits
    "get_customer": RateLimit(max_calls=30, window_seconds=60),
    "search_products": RateLimit(max_calls=20, window_seconds=60),
    "get_order": RateLimit(max_calls=30, window_seconds=60),
    
    # Write operations ‚Äî moderate limits
    "update_order": RateLimit(max_calls=10, window_seconds=60),
    "create_ticket": RateLimit(max_calls=5, window_seconds=60),
    
    # Expensive operations ‚Äî strict limits
    "send_email": RateLimit(max_calls=3, window_seconds=60),
    "charge_payment": RateLimit(max_calls=2, window_seconds=300),
    
    # Dangerous operations ‚Äî very strict
    "delete_data": RateLimit(max_calls=1, window_seconds=600),
}


class ToolRateLimiter:
    """Per-tool rate limiter using sliding window."""
    
    def __init__(self):
        # Track call timestamps: tool_name -> [timestamps]
        self._calls: dict[str, list[datetime]] = defaultdict(list)
    
    def check(self, tool_name: str) -> dict | None:
        """
        Check if a tool call is allowed.
        Returns None if allowed, error dict if rate limited.
        """
        limit = TOOL_RATE_LIMITS.get(tool_name)
        if limit is None:
            # No limit configured ‚Äî apply default
            limit = RateLimit(max_calls=10, window_seconds=60)
        
        now = datetime.utcnow()
        window_start = now - timedelta(seconds=limit.window_seconds)
        
        # Clean old entries
        self._calls[tool_name] = [
            ts for ts in self._calls[tool_name]
            if ts > window_start
        ]
        
        # Check limit
        if len(self._calls[tool_name]) >= limit.max_calls:
            oldest = self._calls[tool_name][0]
            retry_after = (
                oldest + timedelta(seconds=limit.window_seconds) - now
            ).total_seconds()
            
            return {
                "error": "Rate limit exceeded",
                "tool": tool_name,
                "limit": str(limit),
                "retry_after_seconds": round(retry_after, 1),
                "message": (
                    f"Too many calls to {tool_name}. "
                    f"Please wait {round(retry_after)}s before trying again."
                )
            }
        
        # Allow the call
        self._calls[tool_name].append(now)
        return None
```

### Using the rate limiter

```python
rate_limiter = ToolRateLimiter()

def execute_tool(tool_name: str, args: dict) -> dict:
    """Execute a tool with rate limiting."""
    
    # Check rate limit
    limit_error = rate_limiter.check(tool_name)
    if limit_error:
        return limit_error
    
    # Execute the tool
    handler = get_handler(tool_name)
    return handler(**args)
```

---

## Per-user rate limits

Rate limits should also apply per user to prevent one user from consuming all resources:

```python
class UserRateLimiter:
    """Rate limiter scoped to individual users."""
    
    def __init__(self):
        self._calls: dict[str, list[datetime]] = defaultdict(list)
    
    def check(
        self,
        user_id: str,
        tool_name: str,
        limit: RateLimit
    ) -> dict | None:
        """Check per-user, per-tool rate limit."""
        key = f"{user_id}:{tool_name}"
        now = datetime.utcnow()
        window_start = now - timedelta(seconds=limit.window_seconds)
        
        # Clean and check
        self._calls[key] = [
            ts for ts in self._calls[key]
            if ts > window_start
        ]
        
        if len(self._calls[key]) >= limit.max_calls:
            return {
                "error": "Per-user rate limit exceeded",
                "tool": tool_name,
                "user": user_id
            }
        
        self._calls[key].append(now)
        return None


# Combine per-tool and per-user limits
tool_limiter = ToolRateLimiter()
user_limiter = UserRateLimiter()

USER_LIMITS = {
    "free": RateLimit(max_calls=5, window_seconds=60),
    "pro": RateLimit(max_calls=20, window_seconds=60),
    "enterprise": RateLimit(max_calls=100, window_seconds=60),
}

def execute_tool_with_limits(
    tool_name: str,
    args: dict,
    user_id: str,
    user_tier: str
) -> dict:
    """Execute with both per-tool and per-user limits."""
    
    # Check global tool limit
    tool_error = tool_limiter.check(tool_name)
    if tool_error:
        return tool_error
    
    # Check per-user limit
    user_limit = USER_LIMITS.get(user_tier, USER_LIMITS["free"])
    user_error = user_limiter.check(user_id, tool_name, user_limit)
    if user_error:
        return user_error
    
    # Both passed ‚Äî execute
    handler = get_handler(tool_name)
    return handler(**args)
```

---

## Conversation-level call budgets

Limit the total number of tool calls in a single conversation to prevent runaway agentic loops:

```python
@dataclass
class ConversationBudget:
    max_total_calls: int = 50          # Total tool calls per conversation
    max_calls_per_turn: int = 10       # Tool calls per model turn
    max_consecutive_errors: int = 3    # Stop after 3 consecutive errors
    
    calls_made: int = 0
    calls_this_turn: int = 0
    consecutive_errors: int = 0


class ConversationLimiter:
    """Track and enforce conversation-level tool call budgets."""
    
    def __init__(self):
        self._budgets: dict[str, ConversationBudget] = {}
    
    def get_budget(self, conversation_id: str) -> ConversationBudget:
        if conversation_id not in self._budgets:
            self._budgets[conversation_id] = ConversationBudget()
        return self._budgets[conversation_id]
    
    def check(self, conversation_id: str) -> dict | None:
        """Check if the conversation has budget remaining."""
        budget = self.get_budget(conversation_id)
        
        if budget.calls_made >= budget.max_total_calls:
            return {
                "error": "Conversation tool budget exhausted",
                "calls_made": budget.calls_made,
                "max_allowed": budget.max_total_calls,
                "message": (
                    "This conversation has reached its tool call limit. "
                    "Please start a new conversation."
                )
            }
        
        if budget.calls_this_turn >= budget.max_calls_per_turn:
            return {
                "error": "Turn tool budget exhausted",
                "calls_this_turn": budget.calls_this_turn,
                "max_per_turn": budget.max_calls_per_turn,
                "message": "Too many tool calls in this turn."
            }
        
        if budget.consecutive_errors >= budget.max_consecutive_errors:
            return {
                "error": "Too many consecutive errors",
                "consecutive_errors": budget.consecutive_errors,
                "message": (
                    "Multiple tool calls have failed. "
                    "Please describe what you need differently."
                )
            }
        
        return None
    
    def record_call(
        self, conversation_id: str, success: bool
    ) -> None:
        """Record a tool call against the budget."""
        budget = self.get_budget(conversation_id)
        budget.calls_made += 1
        budget.calls_this_turn += 1
        
        if success:
            budget.consecutive_errors = 0
        else:
            budget.consecutive_errors += 1
    
    def new_turn(self, conversation_id: str) -> None:
        """Reset per-turn counters when a new user message arrives."""
        budget = self.get_budget(conversation_id)
        budget.calls_this_turn = 0
```

---

## Token budget management

Tool definitions and results consume tokens. Every tool you register adds to the input token count:

| Provider | Token Impact |
|----------|-------------|
| OpenAI | "Functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens." |
| Gemini | "Function descriptions and parameters count towards your input token limit." |
| Anthropic | System prompt overhead: ~346 tokens (auto/none) + per-tool schema tokens |

```python
def estimate_tool_tokens(tools: list[dict]) -> int:
    """Rough estimate of tokens consumed by tool definitions."""
    import json
    
    # Convert tools to JSON string and estimate tokens
    # Rule of thumb: ~4 characters per token for JSON
    tools_json = json.dumps(tools)
    estimated_tokens = len(tools_json) // 4
    
    # Add Anthropic system prompt overhead
    base_overhead = 350  # ~346 tokens
    
    return estimated_tokens + base_overhead


def select_tools_within_budget(
    all_tools: list[dict],
    context: dict,
    max_tool_tokens: int = 2000
) -> list[dict]:
    """Select tools that fit within a token budget."""
    
    selected = []
    current_tokens = 350  # Base overhead
    
    # Prioritize tools by relevance to current context
    ranked_tools = rank_tools_by_relevance(all_tools, context)
    
    for tool in ranked_tools:
        tool_tokens = len(json.dumps(tool)) // 4
        
        if current_tokens + tool_tokens <= max_tool_tokens:
            selected.append(tool)
            current_tokens += tool_tokens
        else:
            break  # Budget exhausted
    
    return selected
```

### Truncating tool results

Large tool results also consume tokens. Cap them:

```python
def truncate_result(result: dict, max_chars: int = 5000) -> dict:
    """Truncate tool results to fit within token budget."""
    import json
    
    result_str = json.dumps(result)
    
    if len(result_str) <= max_chars:
        return result
    
    # If result has a list, truncate the list
    if "results" in result and isinstance(result["results"], list):
        truncated = dict(result)
        items = truncated["results"]
        
        # Binary search for the right number of items
        while len(json.dumps(truncated)) > max_chars and len(items) > 1:
            items = items[:len(items) // 2]
            truncated["results"] = items
            truncated["_truncated"] = True
            truncated["_showing"] = len(items)
        
        return truncated
    
    # Generic truncation
    return {
        "summary": result_str[:max_chars],
        "_truncated": True,
        "_original_size": len(result_str)
    }
```

---

## Backpressure and graceful degradation

When rate limits are hit, communicate clearly instead of failing silently:

```python
def execute_with_backpressure(
    tool_name: str,
    args: dict,
    context: dict
) -> dict:
    """Execute with rate limiting and graceful degradation."""
    
    # Check all rate limits
    for limiter, check_args in [
        (tool_limiter, (tool_name,)),
        (user_limiter, (context["user_id"], tool_name, get_user_limit(context))),
        (conversation_limiter, (context["conversation_id"],)),
    ]:
        error = limiter.check(*check_args)
        if error:
            # Add guidance for the model
            error["guidance"] = get_degradation_guidance(tool_name, error)
            return error
    
    # Execute with result truncation
    handler = get_handler(tool_name)
    result = handler(**args)
    
    # Truncate large results
    result = truncate_result(result)
    
    # Record the call
    success = "error" not in result
    conversation_limiter.record_call(context["conversation_id"], success)
    
    # Add remaining budget info
    budget = conversation_limiter.get_budget(context["conversation_id"])
    result["_budget_remaining"] = budget.max_total_calls - budget.calls_made
    
    return result


def get_degradation_guidance(tool_name: str, error: dict) -> str:
    """Tell the model what to do when rate limited."""
    
    if "retry_after_seconds" in error:
        return (
            f"The {tool_name} tool is temporarily rate limited. "
            "Tell the user you'll need to wait a moment, "
            "or try a different approach."
        )
    
    if "Conversation tool budget" in error.get("error", ""):
        return (
            "This conversation has reached its tool call limit. "
            "Summarize what you've found so far and suggest "
            "the user start a new conversation for further help."
        )
    
    return "Consider an alternative approach that doesn't require this tool."
```

---

## Best practices

| Practice | Why It Matters |
|----------|----------------|
| Different limits for read vs. write operations | Reads are cheap, writes are expensive |
| Per-user limits based on subscription tier | Prevent one user from consuming all resources |
| Conversation-level call budgets | Prevent runaway agentic loops |
| Monitor token budget from tool definitions | Too many tools can crowd out conversation context |
| Truncate large tool results | Prevents context window overflow |
| Provide clear guidance when rate limited | Model can communicate the delay to the user |

---

## Common pitfalls

| ‚ùå Mistake | ‚úÖ Solution |
|-----------|-------------|
| No rate limiting on tool calls | Add per-tool, per-user, and conversation-level limits |
| Same limit for all tools | Tier limits by operation cost and risk |
| Silent failure when rate limited | Return clear error with retry timing |
| No conversation-level budget | Cap total calls at 50 per conversation |
| Registering 30+ tools without tracking token cost | Estimate token usage and stay within budget |
| Unlimited result sizes flowing back to the model | Truncate to ~5000 characters per result |

---

## Hands-on exercise

### Your task

Build a **multi-level rate limiter** for a customer support chatbot with these tools:

| Tool | Category | Limit |
|------|----------|-------|
| `search_knowledge_base` | Read | 20/minute |
| `get_customer` | Read | 15/minute |
| `get_order` | Read | 15/minute |
| `update_order` | Write | 5/minute |
| `send_email` | External | 3/minute |

### Requirements

1. Implement per-tool rate limiting with the specified limits
2. Add a per-user limit of 30 total calls/minute
3. Add a conversation budget of 40 total calls
4. When rate limited, return a helpful error with `retry_after_seconds`
5. Include a `remaining_budget` field in every successful response

### Expected result

A rate limiter class that correctly enforces all three levels of limits.

<details>
<summary>üí° Hints (click to expand)</summary>

- Use a sliding window based on timestamps, not fixed windows
- The per-user limit is an aggregate across all tools
- Conversation budget doesn't reset ‚Äî it's a lifetime limit
- `retry_after_seconds` should be calculated from the oldest call in the window

</details>

<details>
<summary>‚úÖ Solution (click to expand)</summary>

```python
from datetime import datetime, timedelta
from collections import defaultdict
from dataclasses import dataclass


@dataclass
class Limits:
    per_minute: int

TOOL_LIMITS = {
    "search_knowledge_base": Limits(per_minute=20),
    "get_customer": Limits(per_minute=15),
    "get_order": Limits(per_minute=15),
    "update_order": Limits(per_minute=5),
    "send_email": Limits(per_minute=3),
}

class MultiLevelRateLimiter:
    USER_LIMIT_PER_MINUTE = 30
    CONVERSATION_BUDGET = 40
    
    def __init__(self):
        self._tool_calls: dict[str, list[datetime]] = defaultdict(list)
        self._user_calls: dict[str, list[datetime]] = defaultdict(list)
        self._conv_counts: dict[str, int] = defaultdict(int)
    
    def _clean_window(
        self, timestamps: list[datetime], window_seconds: int = 60
    ) -> list[datetime]:
        cutoff = datetime.utcnow() - timedelta(seconds=window_seconds)
        return [ts for ts in timestamps if ts > cutoff]
    
    def _calc_retry_after(
        self, timestamps: list[datetime], window: int = 60
    ) -> float:
        oldest = min(timestamps)
        available_at = oldest + timedelta(seconds=window)
        return max(0, (available_at - datetime.utcnow()).total_seconds())
    
    def check_and_record(
        self,
        tool_name: str,
        user_id: str,
        conversation_id: str
    ) -> dict | None:
        """Check all limits. Returns error dict or None if allowed."""
        now = datetime.utcnow()
        
        # 1. Conversation budget (lifetime)
        if self._conv_counts[conversation_id] >= self.CONVERSATION_BUDGET:
            return {
                "error": "Conversation budget exhausted",
                "calls_made": self._conv_counts[conversation_id],
                "max_allowed": self.CONVERSATION_BUDGET
            }
        
        # 2. Per-user limit
        user_key = user_id
        self._user_calls[user_key] = self._clean_window(
            self._user_calls[user_key]
        )
        if len(self._user_calls[user_key]) >= self.USER_LIMIT_PER_MINUTE:
            return {
                "error": "Per-user rate limit exceeded",
                "retry_after_seconds": round(
                    self._calc_retry_after(self._user_calls[user_key]), 1
                )
            }
        
        # 3. Per-tool limit
        tool_key = f"{user_id}:{tool_name}"
        self._tool_calls[tool_key] = self._clean_window(
            self._tool_calls[tool_key]
        )
        tool_limit = TOOL_LIMITS.get(
            tool_name, Limits(per_minute=10)
        )
        if len(self._tool_calls[tool_key]) >= tool_limit.per_minute:
            return {
                "error": f"Rate limit exceeded for {tool_name}",
                "limit": f"{tool_limit.per_minute}/minute",
                "retry_after_seconds": round(
                    self._calc_retry_after(self._tool_calls[tool_key]), 1
                )
            }
        
        # All checks passed ‚Äî record the call
        self._tool_calls[tool_key].append(now)
        self._user_calls[user_key].append(now)
        self._conv_counts[conversation_id] += 1
        
        return None
    
    def get_remaining(self, conversation_id: str) -> int:
        return self.CONVERSATION_BUDGET - self._conv_counts[conversation_id]
```

</details>

### Bonus challenges

- [ ] Add a `get_usage_report` method that returns current usage statistics
- [ ] Implement exponential backoff for consecutive rate limit hits
- [ ] Add per-tool cost tracking (assign dollar costs to each tool call)

---

## Summary

‚úÖ **Rate limit every tool** ‚Äî different limits for reads, writes, and external calls

‚úÖ **Per-user limits** prevent one user from consuming all resources

‚úÖ **Conversation budgets** prevent runaway agentic loops (cap at 40-50 calls)

‚úÖ **Monitor token costs** ‚Äî tool definitions and results consume context window

‚úÖ **Truncate large results** to prevent context overflow

‚úÖ **Communicate clearly** when rate limited ‚Äî include retry timing and guidance

**Next:** [Audit Logging ‚Üí](./09-audit-logging.md)

---

[‚Üê Previous: Security Best Practices](./07-security-best-practices.md) | [Back to Lesson Overview](./00-tool-design-best-practices.md)

<!-- 
Sources Consulted:
- OpenAI Function Calling Guide (Token Usage): https://platform.openai.com/docs/guides/function-calling
- Google Gemini Function Calling (Token Limits): https://ai.google.dev/gemini-api/docs/function-calling
- Anthropic Tool Use (Token Overhead): https://platform.claude.com/docs/en/agents-and-tools/tool-use/overview
-->
