---
title: "Unit 25: Fine-Tuning & Custom Models"
---

# Unit 25: Fine-Tuning & Custom Models

## Overview & Importance

Fine-tuning customizes pre-trained models for specific tasks or domains. While RAG and prompting solve many problems, fine-tuning can improve quality, reduce latency, and lower costs for specific use cases. This unit covers when and how to fine-tune, without requiring deep ML expertise.

Fine-tuning matters because:
- Some tasks benefit significantly from customization
- Can reduce prompt sizes (lower cost, faster)
- Improves consistency for specific domains
- Enables capabilities not possible with prompting alone

## Prerequisites

- Strong understanding of LLMs (Unit 2)
- Prompt engineering mastery (Unit 5)
- API integration skills (Unit 3)
- Basic data preparation awareness

## Learning Objectives

By the end of this unit, you will be able to:
- Evaluate when fine-tuning is appropriate
- Prepare datasets for fine-tuning
- Execute fine-tuning through provider APIs
- Evaluate fine-tuned model quality
- Deploy and use fine-tuned models
- Understand costs and trade-offs
- Maintain fine-tuned models over time

## Real-world Applications

- Brand voice customization
- Domain-specific assistants
- Structured data extraction
- Industry-specific terminology
- Consistent formatting outputs
- Cost optimization for high-volume use

## Market Demand & Relevance

- Fine-tuning skills increasingly valued
- Enterprise customization needs
- Differentiates from basic AI integration
- Opens path to ML engineering roles
- High-value consulting opportunity
- Growing as models become more tunable

## Resources & References

- OpenAI Fine-tuning Documentation
  - https://platform.openai.com/docs/guides/fine-tuning
- OpenAI DPO & RFT Documentation
  - https://platform.openai.com/docs/guides/dpo
  - https://platform.openai.com/docs/guides/reinforcement-fine-tuning
- Hugging Face TRL Library
  - https://huggingface.co/docs/trl
- Hugging Face PEFT Library
  - https://huggingface.co/docs/peft
- Unsloth Documentation
  - https://unsloth.ai/docs
- Axolotl Documentation
  - https://docs.axolotl.ai
- Together.ai Fine-tuning
  - https://docs.together.ai/docs/fine-tuning
- Google Vertex AI Fine-tuning
  - https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models
- AWS Bedrock Customization
  - https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html
- LoRA Original Paper
  - https://arxiv.org/abs/2106.09685
- DPO Paper
  - https://arxiv.org/abs/2305.18290
- GRPO / DeepSeek-R1 Paper
  - https://arxiv.org/abs/2401.02954
- Argilla Data Annotation
  - https://docs.argilla.io
- Distilabel Synthetic Data
  - https://distilabel.argilla.io
- Flash Attention
  - https://github.com/Dao-AILab/flash-attention
- Liger Kernel
  - https://github.com/linkedin/Liger-Kernel
